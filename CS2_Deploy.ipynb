{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8aC88ha3w49"
   },
   "source": [
    "**Import all the libraries :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltqL-ylI3V_a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import time\n",
    "import tarfile\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import concat\n",
    "from tensorflow import repeat\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.transform import resize\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from tensorflow.keras.models import Model\n",
    "from google.colab.patches import cv2_imshow\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.backend import expand_dims \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import concatenate, Concatenate\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM, Layer, Dropout, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBEqRtmQrDMK",
    "outputId": "be66b357-b67e-451d-94a0-90dffe5a313a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask-ngrok\n",
      "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
      "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
      "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
      "Installing collected packages: flask-ngrok\n",
      "Successfully installed flask-ngrok-0.0.25\n"
     ]
    }
   ],
   "source": [
    "!pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og-NTHDpr6J-"
   },
   "outputs": [],
   "source": [
    "import flask\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from werkzeug.utils import secure_filename\n",
    "from flask import Flask, redirect, url_for, request, render_template, jsonify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erjkLGO-VhWs"
   },
   "source": [
    "**Creating directories for flask :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSU4zXGyr5jo"
   },
   "outputs": [],
   "source": [
    "os.mkdir('templates')\n",
    "os.mkdir('static')\n",
    "os.mkdir('static/css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI-aNg7tsVEa"
   },
   "source": [
    "**Loading CheXNet Model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agIuJ48vqmVz",
    "outputId": "2560eb89-42f5-4ac5-d451-d70afe53e060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/content/drive/MyDrive/CheXNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LnJMSIFhZtd"
   },
   "source": [
    "**Load structured data :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkMCVlaagwSd"
   },
   "outputs": [],
   "source": [
    "train = np.load('/content/drive/MyDrive/train.npy',allow_pickle=True)\n",
    "test = np.load('/content/drive/MyDrive/test.npy',allow_pickle=True)\n",
    "validation = np.load('/content/drive/MyDrive/validation.npy',allow_pickle=True)\n",
    "\n",
    "columns = [\"front X-Ray\", \"lateral X-Ray\", \"findings\", \"dec_ip\", \"dec_op\", \"image_features\"]\n",
    "\n",
    "train = pd.DataFrame(train, columns = columns)\n",
    "test = pd.DataFrame(test, columns = columns)\n",
    "validation = pd.DataFrame(validation, columns = columns)\n",
    "\n",
    "#Reshaping the Image tensors for training\n",
    "train_image_features = np.vstack(train.image_features).astype(np.float)\n",
    "validation_image_features = np.vstack(validation.image_features).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu-fggCN3yvT"
   },
   "source": [
    "**Text Tokenization :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2V2deQbOua_O"
   },
   "outputs": [],
   "source": [
    "token = Tokenizer( filters='!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "token.fit_on_texts(train['findings'])\n",
    "\n",
    "token.word_index['<pad>'] = 0\n",
    "token.index_word[0] = '<pad>'\n",
    "vocab_size = len(token.word_index) + 1\n",
    "\n",
    "#sequence in train and validation\n",
    "train_inp_dec = token.texts_to_sequences(train.dec_ip)\n",
    "train_op_dec = token.texts_to_sequences(train.dec_op)\n",
    "val_inp_dec = token.texts_to_sequences(validation.dec_ip)\n",
    "val_op_dec = token.texts_to_sequences(validation.dec_op)\n",
    "\n",
    "#padding in the train and validation \n",
    "max_len = 100\n",
    "decoder_input = pad_sequences(train_inp_dec, maxlen=max_len, padding='post')\n",
    "decoder_output =  pad_sequences(train_op_dec, maxlen=max_len, padding='post') \n",
    "Validation_decoder_input = pad_sequences(val_inp_dec, maxlen=max_len, padding='post') \n",
    "Validation_decoder_output = pad_sequences(val_op_dec, maxlen=max_len, padding='post')\n",
    "\n",
    "word_idx = {}\n",
    "idx_word = {}\n",
    "for key, value in (token.word_index).items(): \n",
    "    word_idx[key] = value\n",
    "    idx_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArjVDOV1Ab3f"
   },
   "outputs": [],
   "source": [
    "batch_size     = 50\n",
    "Buffer_size    = 500\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_image_features, decoder_input), decoder_output))\n",
    "train_dataset = train_dataset.shuffle(Buffer_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices(((validation_image_features,Validation_decoder_input),Validation_decoder_output))\n",
    "validation_dataset = validation_dataset.shuffle(Buffer_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PqYTZirHsLs"
   },
   "source": [
    "**Loading Model Architacture :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9jJZEbPQTYK"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self,lstm_units):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        self.dense      = Dense(self.lstm_units, kernel_initializer=\"glorot_uniform\", name = 'encoder_dense_layer')\n",
    "        \n",
    "    def initialize_states(self,batch_size):\n",
    "      \n",
    "        self.batch_size  = batch_size\n",
    "        self.enc_h       = tf.zeros((self.batch_size, self.lstm_units))\n",
    "      \n",
    "        return self.enc_h\n",
    "    \n",
    "    def call(self,x):\n",
    "      \n",
    "        # x : image_data\n",
    "        encoder_output = self.dense(x)\n",
    "      \n",
    "        return encoder_output  \n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Class that calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "    '''\n",
    "    def __init__(self,attention_units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_units = attention_units  \n",
    "\n",
    "        self.w1_Dense    =  tf.keras.layers.Dense(self.attention_units, kernel_initializer=\"glorot_uniform\", name='Concat_w1_Dense')\n",
    "        self.w2_Dense    =  tf.keras.layers.Dense(self.attention_units, kernel_initializer=\"glorot_uniform\", name='Concat_w2_Dense')\n",
    "        self.Concat_Dense=  tf.keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\", name = 'Concat_Dense_layer')\n",
    "  \n",
    "    def call(self,x):\n",
    "    \n",
    "        self.decoder_hidden_state, self.encoder_output = x\n",
    "        self.decoder_hidden_state = tf.expand_dims(self.decoder_hidden_state,axis = 1)\n",
    "    \n",
    "        score = self.Concat_Dense(tf.nn.tanh(self.w1_Dense(self.decoder_hidden_state) + self.w2_Dense(self.encoder_output)))\n",
    "    \n",
    "        att_weights    = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = att_weights * self.encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)   \n",
    "    \n",
    "        return context_vector,att_weights\n",
    "\n",
    "\n",
    "class OneStepDecoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units, attention_units):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.lstm_units     = lstm_units\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.attention_units= attention_units\n",
    "      \n",
    "        self.dense       = Dense(self.vocab_size, kernel_initializer=\"glorot_uniform\", name ='onestep_dense')\n",
    "        self.attention   = Attention( self.attention_units)\n",
    "        self.decoder_emb = Embedding(self.vocab_size, self.embedding_dim, trainable = True , name = 'Decoder_embedding')           \n",
    "        self.decoder_gru = GRU(self.lstm_units, return_state=True, return_sequences=True, name=\"Decoder_LSTM\") \n",
    "      \n",
    "      \n",
    "        self.dropout1 = Dropout(0.3,name = 'dropout1')\n",
    "        self.dropout2 = Dropout(0.3,name = 'dropout2')\n",
    "        self.dropout3 = Dropout(0.3,name = 'dropout3')\n",
    "  \n",
    "    @tf.function\n",
    "    def call(self,x,training=None):\n",
    "    \n",
    "        self.input_to_decoder, self.encoder_output, self.state_h = x\n",
    "\n",
    "        embedded_output = self.decoder_emb(self.input_to_decoder)\n",
    "        embedded_output = self.dropout1(embedded_output)\n",
    "    \n",
    "        y = [self.state_h,self.encoder_output]\n",
    "        context_vector, att_weights = self.attention(y)\n",
    "\n",
    "        concated_decoder_input = tf.concat([tf.expand_dims(context_vector, 1),embedded_output], -1)\n",
    "        concated_decoder_input = self.dropout2(concated_decoder_input)\n",
    "\n",
    "        output_gru, hidden_state = self.decoder_gru(concated_decoder_input, initial_state=self.state_h)\n",
    "    \n",
    "        output_gru = tf.reshape(output_gru, (-1, output_gru.shape[2]))\n",
    "        output_gru = self.dropout3(output_gru)\n",
    "\n",
    "        output = self.dense(output_gru)\n",
    "\n",
    "        return output,hidden_state,att_weights,context_vector\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units, attention_units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm_units     = lstm_units\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.attention_units= attention_units\n",
    "      \n",
    "        self.onestepdecoder = OneStepDecoder(self.vocab_size, self.embedding_dim, self.lstm_units, self.attention_units)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x,training=None):\n",
    "        \n",
    "        self.input_to_decoder, self.encoder_output, self.decoder_hidden_state = x\n",
    "        all_outputs = tf.TensorArray(tf.float32,size = self.input_to_decoder.shape[1], name = 'output_arrays' )\n",
    "        \n",
    "        for timestep in tf.range(self.input_to_decoder.shape[1]):\n",
    "          \n",
    "            y = [self.input_to_decoder[:,timestep:timestep+1],self.encoder_output, self.decoder_hidden_state]\n",
    "            output,hidden_state,att_weights,context_vector = self.onestepdecoder(y)\n",
    "          \n",
    "            self.decoder_hidden_state = hidden_state\n",
    "            all_outputs = all_outputs.write(timestep,output)\n",
    "        \n",
    "        all_outputs = tf.transpose(all_outputs.stack(),[1,0,2])\n",
    "        \n",
    "        return all_outputs\n",
    "\n",
    "\n",
    "class Encoder_decoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "     # Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "     # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "     # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "     # return the decoder output\n",
    "  \n",
    "    \"\"\" \n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units, attention_units, batch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size     = vocab_size\n",
    "        self.batch_size     = batch_size\n",
    "        self.lstm_units     = lstm_units\n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.attention_units= attention_units\n",
    "        \n",
    "        self.encoder = Encoder(self.lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units, attention_units)\n",
    "        self.dense   = Dense(self.vocab_size, kernel_initializer=\"glorot_uniform\", name = 'enc_dec_dense')\n",
    "\n",
    "  \n",
    "    def call(self,data):\n",
    "    \n",
    "        self.inputs, self.outputs = data[0], data[1]\n",
    "\n",
    "        self.encoder_hidden = self.encoder.initialize_states(self.batch_size)\n",
    "        self.encoder_output = self.encoder(self.inputs)\n",
    "    \n",
    "        x = [self.outputs,self.encoder_output,self.encoder_hidden]\n",
    "        output = self.decoder(x)\n",
    "    \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2E5wJjfnDXR"
   },
   "source": [
    "**Custom loss function :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHvwot-iewda"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX5JsG5rnMSa"
   },
   "source": [
    "**Load Model :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5ESr2TlgZrY",
    "outputId": "8ec31e89-bf8d-4ed3-f3bb-83b8bf0f2ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 55s 775ms/step - loss: 1.4419 - val_loss: 1.3443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5fbfa64a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_units     = 256\n",
    "embedding_dim  = 300\n",
    "attention_units= 64\n",
    "tf.keras.backend.clear_session()\n",
    "Attention_model = Encoder_decoder(vocab_size,embedding_dim,lstm_units,attention_units,batch_size)\n",
    "Attention_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=loss_function)\n",
    "Attention_model.fit(train_dataset, validation_data=validation_dataset, epochs=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wedxuyFmW5d0"
   },
   "source": [
    "**Loading best pre-trained weights :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjXN_m57gfSV"
   },
   "outputs": [],
   "source": [
    "Attention_model.load_weights('/content/drive/MyDrive/Attention_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GCa65peXUw4"
   },
   "source": [
    "**Prediction, Inference and Deployment :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv0IbADEsDGr"
   },
   "outputs": [],
   "source": [
    "def load_image(img_name):\n",
    "  \"\"\"Loads image in array format\"\"\"\n",
    "\n",
    "  image = Image.open(img_name)\n",
    "  X = np.asarray(image.convert(\"RGB\"))\n",
    "  X = np.asarray(X)\n",
    "  X = preprocess_input(X)\n",
    "  X = resize(X, (256,256,3))\n",
    "  X = np.expand_dims(X, axis=0)\n",
    "  X = np.asarray(X)\n",
    "    \n",
    "  return X\n",
    "\n",
    "def preprocess(image1_path,image2_path):\n",
    "  '''\n",
    "    input -- dataframe(df)\n",
    "    output -- dataframe(df)\n",
    "    process - convert images into 256 X 256, then using CHeXNET model generate tensor(concate two image tensor)\n",
    "  '''\n",
    "  path = '/content/images/'\n",
    "\n",
    "  image_features = []\n",
    "  for i in range(len(image1_path)):\n",
    "\n",
    "    i1 = load_image(image1_path)\n",
    "    i2 = load_image(image2_path)\n",
    "    img1_features = model.predict(i1)\n",
    "    img2_features = model.predict(i2)\n",
    "    img1_features = np.vstack(img1_features).astype(np.float)\n",
    "    img2_features = np.vstack(img2_features).astype(np.float)\n",
    "    \n",
    "    tensor = np.concatenate((img1_features, img2_features), axis=1)\n",
    "\n",
    "  return tensor\n",
    "\n",
    "#Refrence : https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "\n",
    "def predict_report(image1, image2):\n",
    "    '''\n",
    "    Input - two image and image path\n",
    "    output - return medical report of the images\n",
    "    This function takes images and using encoder decoder model\n",
    "    return medical report of the images\n",
    "    The function predicts the sentence using beam search\n",
    "    '''\n",
    "\n",
    "    img_tensor     = preprocess(image1, image2)\n",
    "    image_features = np.vstack(img_tensor).astype(np.float)\n",
    "\n",
    "    result = ''\n",
    "    initial_state  = Attention_model.layers[0].initialize_states(1)\n",
    "    sequences      = [['<start>', initial_state, 0]]\n",
    "\n",
    "    encoder_output       = Attention_model.layers[0](image_features)\n",
    "    decoder_hidden_state = initial_state\n",
    "\n",
    "    max_len = 75\n",
    "    beam_width = 3\n",
    "    finished_seq = []\n",
    "\n",
    "    for i in range(max_len):\n",
    "        new_seq = []\n",
    "        all_probable = []\n",
    "        \n",
    "        for seq,state,score in sequences:\n",
    "\n",
    "            cur_vec = np.reshape(word_idx[seq.split(\" \")[-1]],(1,1))\n",
    "            decoder_hidden_state = state\n",
    "            x = [cur_vec, encoder_output, decoder_hidden_state]\n",
    "            output,hidden_state,att_weights,context_vector = Attention_model.get_layer('decoder').onestepdecoder(x)\n",
    "            output = tf.nn.softmax(output)\n",
    "            top_words = np.argsort(output).flatten()[-beam_width:]\n",
    "            for index in top_words:\n",
    "         \n",
    "                predicted = [seq + ' '+ idx_word[index], hidden_state, score-np.log(np.array(output).flatten()[index])]\n",
    "                all_probable.append(predicted)\n",
    "\n",
    "        sequences = sorted(all_probable, key = lambda l: l[2])[:beam_width]\n",
    "\n",
    "        count = 0\n",
    "        for seq,state,score in sequences:\n",
    "            if seq.split(\" \")[-1] == '<end>':\n",
    "                score = score/len(seq)\n",
    "                finished_seq.append([seq,state,score])\n",
    "                count+=1\n",
    "            else:\n",
    "                new_seq.append([seq,state,score])\n",
    "        \n",
    "        sequences = new_seq\n",
    "        beam_width= beam_width - count\n",
    "        if not sequences:\n",
    "            break        \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if len(finished_seq) >0:\n",
    "          finished_seq = sorted(finished_seq, reverse=True, key = lambda l: l[2])\n",
    "          sequences = finished_seq[-1]\n",
    "          return sequences[0][8:-5]\n",
    "    else:\n",
    "          return new_seq[-1][0]\n",
    "\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "  return flask.render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'] )\n",
    "def predict_caption():\n",
    "  \n",
    "    front_XRay   = request.files['file_1']\n",
    "    lateral_XRay = request.files['file_2']\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    front_XRay_path   = cwd+'/static/'+secure_filename(front_XRay.filename)\n",
    "    lateral_XRay_path = cwd+'/static/'+secure_filename(lateral_XRay.filename)\n",
    "  \n",
    "    front_XRay.save(front_XRay_path)\n",
    "    lateral_XRay.save(lateral_XRay_path)\n",
    "\n",
    "    result = predict_report(front_XRay_path,lateral_XRay_path)\n",
    "    \n",
    "    return flask.render_template('prediction.html', prediction_text=result, front_XRay=secure_filename(front_XRay.filename), lateral_XRay=secure_filename(lateral_XRay.filename))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtKFeuKS7wqo",
    "outputId": "fd367d43-a308-42d2-8332-f800863c1b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Running on http://8c2a613199d3.ngrok.io\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Jun/2021 20:04:33] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:34] \"\u001b[37mGET /static/css/style1.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:34] \"\u001b[37mGET /static/Pexels_Videos.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:35] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:37] \"\u001b[37mGET /static/css/style1.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:38] \"\u001b[37mGET /static/Pexels_Videos.mp4 HTTP/1.1\u001b[0m\" 206 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:04:57] \"\u001b[31m\u001b[1mGET /predict HTTP/1.1\u001b[0m\" 405 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:05:04] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:05:04] \"\u001b[37mGET /static/css/style.css HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:05:05] \"\u001b[37mGET /static/CXR1002_IM-0004-2001.png HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [07/Jun/2021 20:05:05] \"\u001b[37mGET /static/logo.png HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR-Uof-fGO_v"
   },
   "source": [
    "**You can check the deployment video here : https://youtu.be/V-eDn6c_S08**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS2-Deploy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
